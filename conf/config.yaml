defaults:
  - _self_
  - dset: data
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

sacred: 0
amp: 0
model: 'cnnformer'
name: "a"
batch_size: 8
epochs: 1000
dropout: 0.5
usebn: 0
relutype: "relu"
train: 1
preload: True
weight_decay: 1e-3 # 0.0016436384483285403 1e-3
lr:  1e-3  #  0.0003572489575277796   fine_tune:初始 5e-5,3e-5 3e-5
history_file: params.json
gpu_device: "cuda:0"
device:
CNN_avg_pool_width: 10
CNN_num_channels: 50
CNN_dilation: 2
CNN_kernel_size: 5
avg_pool: True
dropout_percent: 0.2

task: hyperimg  # rgb | hyper | both | VAE
data_clean: 1
embedding: 0
optim: Adam
output_root: outputs/
lr_scheduler: CyclicLR
channel_num: 4
prob: 0.2
num_masks: 1
attention: ChannelSELayer
leafwidth: mean
indice_num: 19
fine_tune: False
seed: 1000
pretrain_name: aaa
label_name: Chl
gl: 0
resample: 204
compress_num: 1
# Hydra config
hydra:
  run:
    dir: ${output_root}/exp_${name}
  job:
    config:
      # configuration for the ${hydra.job.override_dirname} runtime variable
      override_dirname:
        kv_sep: '='
        item_sep: ','
        # Remove all paths, as the / in them would mess up things
        # Remove params that would not impact the training itself
        # Remove all slurm and submit params.
        # This is ugly I know...
        exclude_keys: [
          'hydra.job_logging.handles.file.filename',
          'dset',
          'dropout',

        ]
  job_logging:
    handlers:
      file:
        class: logging.FileHandler
        mode: a
        #formatter: colorlog
        filename: trainer${train}.log
      console:
        class: logging.StreamHandler
        #formatter: colorlog
        stream: ext://sys.stderr

  hydra_logging:
    handlers:
      console:
        class: logging.StreamHandler
        #formatter: colorlog
        stream: ext://sys.stderr
